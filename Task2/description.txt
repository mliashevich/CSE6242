EntryPoint
Производим все те же действия, что и для hadoop. Map - распаршиваем строку и возращаем пару nodeId, weight
на следующем шаге делаем reduce, где суммируем веса для одной вершины.
На этом шаге можно было и остановиться, но spark имеет "специфичный" вывод,
поэтому отсортируем по ключу и преобразуем сторку вывода к следующему виду: nodeId  totalWeight, для удобного чтения.

Посмотрим время обратоки одних и тех же файлов с использованием hadoop/java и spark/scala

graph1.tsv
spark/scala:
real    36.7s
hadoop/java:
real    1m 9.4s

spark/scala:
real    2m 28.7s
hadoop/java:
real    10m 6.4s

Как и ожидалось, скорость обработки данных в spark в разы превышает обработку, их же, с использованием hadoop.

