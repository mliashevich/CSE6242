EntryPoint
Производим все те же действия, что и для hadoop. Map - распаршиваем строку и возращаем пару nodeId, weight
на следующем шаге делаем reduce, где суммируем веса для одной вершины.
На этом шаге можно было и остановиться, но spark имеет "специфичный" вывод,
поэтому отсортируем по ключу и преобразуем строку вывода к следующему виду: nodeId  totalWeight, для удобного чтения.

Посмотрим время обратоки одних и тех же файлов с использованием hadoop/java и spark/scala

graph1.tsv
spark/scala: 28.2s
hadoop/java: 1m 9.4s

graph2.tsv
spark/scala: 2m 16.7s
hadoop/java: 10m 6.4s

Как и ожидалось, скорость обработки данных в spark в разы превышает обработку, их же, с использованием hadoop.

